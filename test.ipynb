{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47eeafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 593994 entries, 0 to 593993\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   id                    593994 non-null  int64  \n",
      " 1   annual_income         593994 non-null  float64\n",
      " 2   debt_to_income_ratio  593994 non-null  float64\n",
      " 3   credit_score          593994 non-null  int64  \n",
      " 4   loan_amount           593994 non-null  float64\n",
      " 5   interest_rate         593994 non-null  float64\n",
      " 6   gender                593994 non-null  object \n",
      " 7   marital_status        593994 non-null  object \n",
      " 8   education_level       593994 non-null  object \n",
      " 9   employment_status     593994 non-null  object \n",
      " 10  loan_purpose          593994 non-null  object \n",
      " 11  grade_subgrade        593994 non-null  object \n",
      " 12  loan_paid_back        593994 non-null  float64\n",
      "dtypes: float64(5), int64(2), object(6)\n",
      "memory usage: 58.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ca1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = [\n",
    "    'annual_income','debt_to_income_ratio','credit_score','loan_amount','interest_rate',\n",
    "    'gender','marital_status','education_level','employment_status','loan_purpose',\n",
    "    'grade_subgrade'\n",
    "]\n",
    "\n",
    "X = train_df[features_to_use]\n",
    "y = train_df['loan_paid_back']\n",
    "X_test = test_df[features_to_use]\n",
    "\n",
    "categorical_features = [\n",
    "    'gender','marital_status','education_level','employment_status','loan_purpose',\n",
    "    'grade_subgrade'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce44d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xgboost_features_simple(X, X_test=None):\n",
    "    X_xgb = X.copy()\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test_xgb = X_test.copy()\n",
    "    else:\n",
    "        X_test_xgb=None\n",
    "\n",
    "    \n",
    "    categorical_cols = ['gender','marital_status','education_level','employment_status','loan_purpose',\n",
    "    'grade_subgrade']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_xgb[col]=le.fit_transform(X_xgb[col].astype(str))\n",
    "        if X_test is not None:\n",
    "            unique_train = set(le.classes_)\n",
    "            X_test_xgb[col] = X_test_xgb[col].astype(str).apply(\n",
    "                lambda x: le.transform([x])[0] if x in unique_train else -1\n",
    "            )\n",
    "    \n",
    "    if X_test is not None:\n",
    "        return X_xgb, X_test_xgb\n",
    "    return X_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6093bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 475195\n",
      "Validation samples: 118799\n",
      "Training models...\n",
      "Training CatBoost... Training models...\n",
      "Training CatBoost... ✓\n",
      "Training XGBoost... ✓\n",
      "Training XGBoost... ✓\n",
      "Training LightGBM... ✓\n",
      "Training LightGBM... ✓\n",
      "\n",
      "All models trained successfully!\n",
      "✓\n",
      "\n",
      "All models trained successfully!\n",
      "\n",
      "Predictions ready for ensemble!\n",
      "\n",
      "Predictions ready for ensemble!\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "\n",
    "# Preparing data for all models\n",
    "X_train_cat = X_train.copy()\n",
    "X_val_cat = X_val.copy()\n",
    "X_test_cat = X_test.copy()\n",
    "\n",
    "# Preparing XGBoost features (label encoded)\n",
    "X_train_xgb, X_val_xgb = prepare_xgboost_features_simple(X_train, X_val)\n",
    "X_test_xgb = prepare_xgboost_features_simple(X_test)\n",
    "\n",
    "# For LightGBM model\n",
    "X_train_lgb, X_val_lgb = prepare_xgboost_features_simple(X_train, X_val)\n",
    "X_test_lgb = prepare_xgboost_features_simple(X_test)\n",
    "\n",
    "# CatBoost model\n",
    "cat_model = CatBoostRegressor(\n",
    "    cat_features=categorical_features,\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    iterations=800,  \n",
    "    learning_rate=0.03,  \n",
    "    depth=8,  \n",
    "    l2_leaf_reg=3,  \n",
    "    random_strength=0.5, \n",
    "    bagging_temperature=0.8,  \n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=800,  \n",
    "    learning_rate=0.03,\n",
    "    max_depth=8, \n",
    "    subsample=0.85,  \n",
    "    colsample_bytree=0.8,\n",
    "    colsample_bylevel=0.8,  \n",
    "    reg_alpha=0.2,  \n",
    "    reg_lambda=0.3,  \n",
    "    gamma=0.1,  \n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=50,\n",
    "    verbosity=0  \n",
    ")\n",
    "\n",
    "# LightGBM model\n",
    "lgb_model = LGBMRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=800,  \n",
    "    learning_rate=0.03,  \n",
    "    max_depth=8,  \n",
    "    num_leaves=45,  \n",
    "    subsample=0.85,  \n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,  \n",
    "    reg_lambda=0.3,  \n",
    "    min_child_samples=25,  \n",
    "    min_child_weight=0.001, \n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "\n",
    "# Model training\n",
    "print(\"Training models...\")\n",
    "\n",
    "print(\"Training CatBoost...\", end=\" \")\n",
    "cat_model.fit(X_train_cat, y_train, eval_set=[(X_val_cat, y_val)], verbose=False)\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"Training XGBoost...\", end=\" \")\n",
    "xgb_model.fit(X_train_xgb, y_train, eval_set=[(X_val_xgb, y_val)], verbose=False)\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"Training LightGBM...\", end=\" \")\n",
    "lgb_model.fit(X_train_lgb, y_train)\n",
    "print(\"✓\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")\n",
    "\n",
    "# Base predictions\n",
    "cat_val_pred = cat_model.predict(X_val_cat)\n",
    "cat_test_pred = cat_model.predict(X_test_cat)\n",
    "\n",
    "xgb_val_pred = xgb_model.predict(X_val_xgb)\n",
    "xgb_test_pred = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "lgb_val_pred = lgb_model.predict(X_val_lgb)\n",
    "lgb_test_pred = lgb_model.predict(X_test_lgb)\n",
    "\n",
    "print(\"\\nPredictions ready for ensemble!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55432923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stacking ensemble...\n",
      "Stacking feature shapes:\n",
      "Train: (475195, 3)\n",
      "Val: (118799, 3)\n",
      "Test: (254569, 3)\n",
      "Stacking ensemble training completed!\n",
      "Stacking feature shapes:\n",
      "Train: (475195, 3)\n",
      "Val: (118799, 3)\n",
      "Test: (254569, 3)\n",
      "Stacking ensemble training completed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"Creating stacking ensemble...\")\n",
    "\n",
    "# Getting predictions from all models\n",
    "cat_train_pred = cat_model.predict(X_train_cat)\n",
    "cat_val_pred = cat_model.predict(X_val_cat)\n",
    "cat_test_pred = cat_model.predict(X_test_cat)\n",
    "\n",
    "xgb_train_pred = xgb_model.predict(X_train_xgb)\n",
    "xgb_val_pred = xgb_model.predict(X_val_xgb)\n",
    "xgb_test_pred = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "lgb_train_pred = lgb_model.predict(X_train_lgb)\n",
    "lgb_val_pred = lgb_model.predict(X_val_lgb)\n",
    "lgb_test_pred = lgb_model.predict(X_test_lgb)\n",
    "\n",
    "# Create stacking features (use all 3 models)\n",
    "level1_train = np.column_stack([cat_train_pred, xgb_train_pred, lgb_train_pred])\n",
    "level1_val = np.column_stack([cat_val_pred, xgb_val_pred, lgb_val_pred])\n",
    "level1_test = np.column_stack([cat_test_pred, xgb_test_pred, lgb_test_pred])\n",
    "\n",
    "print(f\"Stacking feature shapes:\")\n",
    "print(f\"Train: {level1_train.shape}\")\n",
    "print(f\"Val: {level1_val.shape}\") \n",
    "print(f\"Test: {level1_test.shape}\")\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = Ridge(alpha=0.1)\n",
    "meta_model.fit(level1_train, y_train)\n",
    "\n",
    "# Make stacking predictions\n",
    "stacking_val_pred = meta_model.predict(level1_val)\n",
    "stacking_test_pred = meta_model.predict(level1_test)\n",
    "\n",
    "print(\"Stacking ensemble training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b790ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate individual model performances with multiple metrics\n",
    "cat_val_rmse = np.sqrt(mean_squared_error(y_val, cat_val_pred))\n",
    "xgb_val_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_pred))\n",
    "lgb_val_rmse = np.sqrt(mean_squared_error(y_val, lgb_val_pred))\n",
    "stack_val_rmse = np.sqrt(mean_squared_error(y_val, stacking_val_pred))\n",
    "\n",
    "cat_mae = mean_absolute_error(y_val, cat_val_pred)\n",
    "xgb_mae = mean_absolute_error(y_val, xgb_val_pred)\n",
    "lgb_mae = mean_absolute_error(y_val, lgb_val_pred)\n",
    "\n",
    "# Combined score (RMSE + MAE)\n",
    "def combined_score(rmse, mae):\n",
    "    return 0.7 * rmse + 0.3 * mae  # Weight RMSE more heavily\n",
    "\n",
    "cat_score = combined_score(cat_val_rmse, cat_mae)\n",
    "xgb_score = combined_score(xgb_val_rmse, xgb_mae)\n",
    "lgb_score = combined_score(lgb_val_rmse, lgb_mae)\n",
    "stack_score = combined_score(stack_val_rmse, 0)\n",
    "\n",
    "# Performance-based weights with exponential decay (better models get much higher weight)\n",
    "models_scores = {\n",
    "    'CatBoost': cat_score,\n",
    "    'XGBoost': xgb_score,\n",
    "    'LightGBM': lgb_score,\n",
    "    'Stacking': stack_score\n",
    "}\n",
    "\n",
    "# Exponential weighting (emphasizes differences between models)\n",
    "weights = {}\n",
    "total_weight = 0\n",
    "for name, score in models_scores.items():\n",
    "    weights[name] = np.exp(-score * 5) \n",
    "    total_weight += weights[name]\n",
    "\n",
    "# Normalize weights\n",
    "for name in weights:\n",
    "    weights[name] /= total_weight\n",
    "\n",
    "print(\"\\nSmart Model Weights:\")\n",
    "for name, weight in weights.items():\n",
    "    print(f\"{name}: {weight:.3f}\")\n",
    "\n",
    "# Super ensemble with smart weighting\n",
    "super_ensemble_val = (\n",
    "    weights['CatBoost'] * cat_val_pred +\n",
    "    weights['XGBoost'] * xgb_val_pred +\n",
    "    weights['LightGBM'] * lgb_val_pred +\n",
    "    weights['Stacking'] * stacking_val_pred\n",
    ")\n",
    "\n",
    "super_ensemble_test = (\n",
    "    weights['CatBoost'] * cat_test_pred +\n",
    "    weights['XGBoost'] * xgb_test_pred +\n",
    "    weights['LightGBM'] * lgb_test_pred +\n",
    "    weights['Stacking'] * stacking_test_pred\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
